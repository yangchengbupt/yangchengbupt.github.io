name: Crawl Google Scholar Base

on:
  schedule:
    - cron: '0 7 * * *'
  workflow_dispatch:

jobs:
  crawl:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      - name: Install Reqs
        run: sudo apt-get install -y python3-setuptools
      - name: Run main (base fetch only)
        env:
          GOOGLE_SCHOLAR_ID: ${{ secrets.GOOGLE_SCHOLAR_ID }}
        run: |
          cd ./google_scholar_crawler
          pip3 install -r requirements.txt
          python3 main.py
          cd ./results
          git init
          git config --local user.name "${{ github.repository_owner }}"
          git config --local user.email "${{ github.repository_owner }}@users.noreply.github.com"
          export remote_repo="https://${GITHUB_ACTOR}:${{ secrets.GITHUB_TOKEN }}@github.com/${GITHUB_REPOSITORY}.git"
          git add gs_data.json gs_data_shieldsio.json || true
          git commit -m "Updated base Google Scholar data"
          git push "${remote_repo}" HEAD:google-scholar-stats-base --force
